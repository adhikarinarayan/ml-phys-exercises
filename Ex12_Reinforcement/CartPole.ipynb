{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0282ff4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1375eef17bc226622bf2b6e11084829a",
     "grade": false,
     "grade_id": "cell-3a080b8ec0f6d338",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import gymnasium as gym\n",
    "import hashlib\n",
    "from movies import Movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34804a80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b61e7d8790217d1e9debd026a9565fa",
     "grade": false,
     "grade_id": "cell-a2e418883ee92544",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Exercise 12: The cart-pole problem\n",
    "==================================\n",
    "<img src=\"cart_pole_figure.png\" style=\"max-width:50%; float:right; padding-left:30pt\">\n",
    "\n",
    "We are looking at a problem of how to balance a pole fixed onto a cart (see Figure). The cart can either move along a fixed axis from left to right. The pole is attached to the top of the cart, but is able to tilt (rotate) to the left and the right due to gravity and the movement of the cart.\n",
    "\n",
    "At each time step the driver of the cart has to make a decision whether to accelerate the cart to the left or the right (by some constant force).  To make this decision, the cart driver can observe the following: position $x$ and velocity $\\dot x$ of the cart as well as angle $\\theta$ and angular velocity $\\dot\\theta$ of the pole ($\\theta = 0$ corresponds to a vertical pole). The goal of the driver is to maximize the time the pole stays reasonably upright, i.e., within $\\theta \\in [-12\\degree, 12\\degree]$, their total compensation is proportional to that time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa9eb3d-75a1-432f-90fd-4e7f4e489bd4",
   "metadata": {},
   "source": [
    "Reinforcement learning problem\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499a5658-7eb2-4307-9080-b74e14f45486",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "079aa14988d66ff4727c0bf712cec55d",
     "grade": false,
     "grade_id": "cell-9155fd4cafe82418",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "As a first step, let us cast the above description in the language of reinforcement learning.\n",
    "\n",
    "Specify for the problem:\n",
    " 1. observation space\n",
    " 2. action space\n",
    " 3. suitable reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1643d-ce8a-49b7-9604-5bd8be0c22d5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ec4cc6fe00f4083c658d8ac398fb193",
     "grade": true,
     "grade_id": "cell-a15c9de9dfbd9554",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cf3aef-7d5a-4078-8eb8-fad45f775dff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ee3db1f0be4aa5bda9ccd4c117e5664",
     "grade": false,
     "grade_id": "cell-5b47357e3b695881",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Also answer the following questions for this problem:\n",
    "\n",
    "  1. Can the state space be chosen identical to the observation space for this problem? Why or why not?\n",
    "  2. Is the transition function deterministic or stochastic?\n",
    "  3. Is the process Markovian?\n",
    "\n",
    "Optional extra points:\n",
    "  - How would the above asked points change if we would not observe the (angular-) velocity, but only the absolute position and angle? \n",
    "  - How would that change the approach we need to take? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c925175b-d0b2-4b16-83bd-105a2952a983",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b3529f8f526981186d8acf9f35697aa",
     "grade": true,
     "grade_id": "cell-4180c8f253bb305e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46867c-133a-4829-b5e1-a6b2426cde35",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49a510c2159511560ea726d27b619fe8",
     "grade": false,
     "grade_id": "cell-4661dd2397e9a36d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Setting up a gymnasium environment\n",
    "----------------------------------\n",
    "We are going to use the [gymnasium] package, which provides the possiblity to model a wide range of environments, with which we can have agents interact with, and train on.\n",
    "\n",
    "The [CartPole] is supported there as a standard environment. \n",
    "The following line constructs a new environment for the cart-pole:\n",
    "\n",
    "[gymnasium]: https://gymnasium.farama.org/\n",
    "[CartPole]: https://gymnasium.farama.org/environments/classic_control/cart_pole/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d363dc4-f58c-4f88-8cb9-1d188ac6e1b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca67c9760ab1ec5bae11c003bfde7491",
     "grade": false,
     "grade_id": "cell-98e48db8e2aff0a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd795ae4-5ffe-46f4-9530-3d4706e58247",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f41ecb39db777bab6b8d15920a9226fa",
     "grade": false,
     "grade_id": "cell-af0c902b1d44cbea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Each [environment] has a set of methods how we interact with it. The important ones are:\n",
    "\n",
    " - `env.reset()` resets the environment to some random initial condition and returns the inital observation\n",
    " - `env.step(action)` takes an `action` and updates the environment accordingly, returning the reward and the updated observations (and some other stuff).\n",
    "\n",
    "Below you find a minimal version of a such a simulation, where we always simply push the cart to\n",
    "the left:\n",
    "\n",
    "[environment]: https://gymnasium.farama.org/api/env/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff019ad7-ffbb-476a-9241-021a6555b3f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1dceec63da224cebd7db4402ba15351",
     "grade": false,
     "grade_id": "cell-b6c72b3b6af49ee1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Movie instance to capture the thing\n",
    "movie = Movie()\n",
    "\n",
    "# Inititalize environment\n",
    "# observation is an array of four numbers [x, xdot, theta, thetadot]\n",
    "observation, info = env.reset()\n",
    "terminated = truncated = False\n",
    "steps = 0\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    # action: 0 = push cart left, 1 = push cart right\n",
    "    action = 0\n",
    "    \n",
    "    # Take a step\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "    # Add to movie\n",
    "    movie.add_state(env)\n",
    "\n",
    "print(f\"run ended after {steps} out of a possible 500 steps\")\n",
    "movie.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cf457-ce83-4c85-810e-c7e18b7e321f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccbba5e152683154a6fd36c08b4eaed7",
     "grade": false,
     "grade_id": "cell-4c82a557c4703075",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "This is less than impressive, so let's try to improve on that policy.\n",
    "\n",
    "Try out the following\n",
    "\n",
    " 1. first simply push the cart left whenever the pole is pointing\n",
    "    left and right if the pole is pointing right. Observe that this\n",
    "    results in an \"increasing oscillating motion\"\n",
    "\n",
    " 2. we can improve on this by considering $\\dot\\theta$ rather than $\\theta$.\n",
    "    Observe that this results in the cart \"running off\" the field\n",
    "    of play.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972fda34-5bf1-4c4f-aa6e-a1dd4f9ada7f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbb3fd367554c493c0726c6fb334dde7",
     "grade": false,
     "grade_id": "cell-aea26960d3408761",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Movie instance to capture the thing\n",
    "movie = Movie()\n",
    "\n",
    "# Inititalize environment\n",
    "# observation is an array of four numbers [x, xdot, theta, thetadot]\n",
    "observation, info = env.reset()\n",
    "terminated = truncated = False\n",
    "steps = 0\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    # action: 0 = push cart left, 1 = push cart right\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    # Take a step\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "    # Add to movie\n",
    "    movie.add_state(env)\n",
    "# print(observation, reward, terminated, truncated, info)\n",
    "print(f\"run ended after {steps} out of a possible 500 steps\")\n",
    "\n",
    "movie.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207b22e-ca6a-4992-b975-10a76585efd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a6a031a32517ef6507cc63e696547c8",
     "grade": true,
     "grade_id": "cell-cd7752cd14aa3b57",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.abs(observation[0]) > 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ff5eb-205d-4353-849a-af0d4fb75ef5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f48a1681a203e00d00a7ae38e435730a",
     "grade": false,
     "grade_id": "cell-d07e515c19f62ef8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Q-table learning\n",
    "----------------\n",
    "We are going to try to apply Q-table learning to the problem, and see whether the machine is \n",
    "able to generate a policy which keeps the pole stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465328b3-5073-4e57-9445-1d823cbc86b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08c537dde538d392c72b03e1372d36da",
     "grade": false,
     "grade_id": "cell-f7503c2a0b14ba4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Remember that Q learning works with a \"quality\" table $Q$, where $Q_{ao}$ is the expected return\n",
    "(cumulative reward) when taking the action $a$ when observing $o$. Since, however, our observation\n",
    "space is continuous, we cannot apply Q-learning directly, we must first **discretize** our space.\n",
    "\n",
    "For this, we are going to write a function `obs_to_q_index`, which takes in an `observation`, i.e.,\n",
    "a list $x, \\dot x, \\theta, \\dot\\theta$, and returns an integer index $0, 1, ..., N-1$ we can use\n",
    "to index the Q-table.\n",
    "\n",
    "To do this, we try the following prescription, selected empirically. \n",
    "We only look at $\\theta$ and $\\dot\\theta$:\n",
    "\n",
    " 1. For $\\theta$, the allowed range is about $[-0.42, 0.42]$ (radians, which corresponds to 24 degrees), \n",
    "   and we are going to split that into 6 equisized bins, i.e., we choose $i = 0$ for\n",
    "   $\\theta \\in [-0.42, -0.28)$, $i = 1$ for $\\theta \\in [-0.28, -0.14)$ and so on.\n",
    "   \n",
    " 2. For $\\dot\\theta$, the range is inifinite. We are going to use three bins: $j = 0$ for $\\dot\\theta < -0.29$,\n",
    "    $j=1$ for $\\dot\\theta \\in [-0.29, 0.29]$, and $j = 2$ for $\\dot\\theta > 0.29$.\n",
    "\n",
    "Now the full index returned is then simply $3 i + j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62c0dc8-665c-47b5-9993-64908e02ad5c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a46562b9c980fce3b379ca9947a87a40",
     "grade": false,
     "grade_id": "cell-b7bd8450d76d8d02",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write your obs_to_q_index function here:\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a3847-13ea-4749-9262-572680818e78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3d1318a4189484cf4722d81d9e7a2b9",
     "grade": true,
     "grade_id": "cell-d7c33f746a68fe71",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert obs_to_q_index(np.array([1.1, 0.3, 0.31, 0.17])) == 16\n",
    "assert obs_to_q_index(np.array([1.8, 0.1, 0.0, -0.5])) == 9\n",
    "assert obs_to_q_index(np.array([0.0, -1.0, -0.4, -1.0])) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1149eb-8e55-4856-bd7d-3d72107f285e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fd77edffdd31472eb8fc4afb6c0f2f3",
     "grade": false,
     "grade_id": "cell-d324806019177493",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Next, let's create our Q table.  Make a new variable `q_table` with the\n",
    "suitable dimension for the problem and fill it with zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581449f-2bae-42ab-84a2-5ce8529b4913",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd45c665838c285e8f390abf4609e721",
     "grade": false,
     "grade_id": "cell-f7a35738dc9e1833",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75217369-2358-49c6-9935-48ddb4fc5323",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d160ac9c060a40afaecbfd56fec2ae5f",
     "grade": true,
     "grade_id": "cell-a2a1032cc3591071",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.abs(q_table).sum() == 0\n",
    "assert hashlib.sha256(repr(q_table.shape).encode('utf-8')).hexdigest() == \\\n",
    "    '5acc3b7fb640f861cd6d6627410ca1ff9711fe15a0bac8f67c2e364e17538377', \"wrong shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297fe64-2f89-4fb8-8d99-5eb00ece652e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a5e199573a4068043c70c7a6b459677",
     "grade": false,
     "grade_id": "cell-aaa2d6ab8cf06e77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We need a policy (i.e., a function choosing an action for a given observation and exploration parameter $\\epsilon$)\n",
    "based on the values in the Q-table and some randomness.\n",
    "\n",
    "Fill in this function such that it returns a suitable action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927ed97-68bb-440d-b8fe-696ac927eebb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d240686338ae4129cfc20b12839a5acc",
     "grade": false,
     "grade_id": "cell-e7355864164b40e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(4711)\n",
    "\n",
    "def choose_action(observation, epsilon):\n",
    "    global q_table\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d943e2-3f6c-470c-a865-d7c95bab4b84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d10de8cbb0ffece0a2f0a077ab3296de",
     "grade": true,
     "grade_id": "cell-822b81f0fe8ddd6b",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert choose_action(np.array([1.1, 0.3, -0.31, 0.17]), 0.1) in (0, 1)\n",
    "\n",
    "q_table[16, :] = [0.2, 0.3]\n",
    "assert choose_action(np.array([1.1, 0.3, 0.31, 0.17]), 0.0) == 1\n",
    "q_table[10, :] = [0.5, -0.1]\n",
    "assert choose_action(np.array([1.1, 0.3, 0.0, -0.1]), 0.0) == 0\n",
    "q_table[16, :] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccc65c-4ac5-4d31-9413-0c899fa3dcbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10585d21d7c5647840821985c4e07e37",
     "grade": false,
     "grade_id": "cell-e328ff8ec1ca83e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We need one final function: a way to update the policy. For this, we are going to use\n",
    "the [Bellmann equation]:\n",
    "$$\n",
    "Q_{oa} \\gets (1-\\eta) Q_{oa} + \\eta (r + \\gamma Q^*_{o'})\n",
    "$$\n",
    "where $o$ is the current observation, $a$ is the action, $o'$ is the next observation\n",
    "after we have taken the action, $r$ is the reward, $\\eta$ is the learning rate, and\n",
    "$\\gamma$ is the discount rate (which we fix to some high value).\n",
    "\n",
    "[Bellmann equation]: https://en.wikipedia.org/wiki/Bellman_equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6732c8-8ca4-4361-ac9a-7409ebfc5612",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79dd86958ba01f0b684789a1f05f7252",
     "grade": true,
     "grade_id": "cell-c908bc75967d39ca",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "def update_q(action, obs, reward, next_obs, eta):\n",
    "    global q_table, gamma\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1736d817-220d-4f67-8d96-7ea783e62f09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1aca982-7669-4d26-8af0-f60f22a8c467",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1c7d697aea65e71ffcb2672ade68e8a8",
     "grade": false,
     "grade_id": "cell-a0ee57f4a2417188",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "We are finally ready to train!\n",
    "\n",
    "In the code below, adapt the training loop from the beginning to use the\n",
    "`choose_action` and `update_q` functions from above.  Note that in order\n",
    "to update the Q table, you need access to the observation before and after\n",
    "you've taken the step.\n",
    "\n",
    "Note that it occasionally happen that the training fails even though you\n",
    "did everything correctly. Simply run the cell two or three times to see\n",
    "whether anything changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36948d94-e9ec-4a69-a3fd-9b5c633a672c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86ad9231f206cf1eeb102d4f8245c433",
     "grade": true,
     "grade_id": "cell-21c98e4160214432",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the system\n",
    "q_table[...] = 0\n",
    "last_failure = 0\n",
    "epsilon = 1.0\n",
    "eta = 1.0\n",
    "lengths = []\n",
    "\n",
    "# Train the system\n",
    "print(\"Training: \", end=\"\")\n",
    "for episode_no in range(1000):\n",
    "    print(\".\", end=\"\")\n",
    "    # Learning rate and explore rate diminishes monotonically over time\n",
    "    epsilon = max(0.01, 0.97 * epsilon)\n",
    "    eta = max(0.01, 0.98 * eta)\n",
    "\n",
    "    # (Re-)inititalize environment for each new episode\n",
    "    observation, info = env.reset()\n",
    "    terminated = truncated = False\n",
    "    steps = 0\n",
    "    \n",
    "    # Loop for simulating a system once until done\n",
    "    while not (terminated or truncated):\n",
    "        # Choose action, take step, update policy:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        steps += 1\n",
    "\n",
    "    # Add some diagnostics here\n",
    "    lengths.append(steps)\n",
    "    if steps < 500:\n",
    "        last_failure = episode_no\n",
    "    elif episode_no - last_failure >= 100:\n",
    "        print(f'\\nCartPole problem is solved after {episode_no} eps.')\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "pl.xlabel(\"episode number\")\n",
    "pl.ylabel(\"number of steps\")\n",
    "pl.title(\"training performance\")\n",
    "pl.plot(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90972c-d895-45ef-aa51-b3bf352dc7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f894e398-68bb-433a-8cea-dda9b23ffbb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "928391d998fc4c8992e73adf69789018",
     "grade": false,
     "grade_id": "cell-7cfbef28ee7205ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Finally, let's see our trained cart in action:\n",
    "\n",
    "Use the simulation code from the very beginning and adapt it such that we choose\n",
    "the action based on the trained policy. (How should we choose `epsilon`?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beed62b-28a8-4865-94b5-629a981865e6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3b5b649ae04a75556bf09feb73ca9a68",
     "grade": true,
     "grade_id": "cell-6240e4be283d2768",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a86a54-6d5a-4f42-9a1f-26826cb56759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
